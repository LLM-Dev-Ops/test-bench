# LLM Test Bench Configuration Example
#
# This file shows all available configuration options with their default values.
# Copy this file to ~/.config/llm-test-bench/config.toml and customize as needed.
#
# Configuration Hierarchy (highest to lowest priority):
# 1. CLI Arguments
# 2. Environment Variables (LLM_TEST_BENCH_ prefix)
# 3. Config Files (~/.config/llm-test-bench/config.toml)
# 4. Defaults (shown below)

# ============================================================================
# Provider Configurations
# ============================================================================
# Configure LLM providers (OpenAI, Anthropic, etc.)
# Each provider needs an API key set via environment variable

[providers.openai]
# Environment variable name containing the API key
# Set this with: export OPENAI_API_KEY="sk-..."
api_key_env = "OPENAI_API_KEY"

# Base URL for the OpenAI API
base_url = "https://api.openai.com/v1"

# Default model to use if not specified in requests
# Available: gpt-4, gpt-4-turbo, gpt-4-turbo-preview, gpt-3.5-turbo
default_model = "gpt-4-turbo"

# Request timeout in seconds (1-300)
timeout_seconds = 30

# Maximum retry attempts for failed requests (0-10)
max_retries = 3

# Optional: Rate limit in requests per minute
# Uncomment to enable throttling:
# rate_limit_rpm = 60

[providers.anthropic]
# Environment variable name containing the API key
# Set this with: export ANTHROPIC_API_KEY="sk-ant-..."
api_key_env = "ANTHROPIC_API_KEY"

# Base URL for the Anthropic API
base_url = "https://api.anthropic.com/v1"

# Default model to use if not specified in requests
# Available: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
default_model = "claude-3-sonnet-20240229"

# Request timeout in seconds (1-300)
timeout_seconds = 30

# Maximum retry attempts for failed requests (0-10)
max_retries = 3

# Optional: Rate limit in requests per minute
# rate_limit_rpm = 50

# ============================================================================
# Benchmark Configuration
# ============================================================================
# Controls how benchmarks are executed

[benchmarks]
# Directory where benchmark results will be saved
# Can be absolute or relative path
output_dir = "./bench-results"

# Whether to save full LLM responses to disk
# Warning: Can consume significant disk space for large benchmark suites
save_responses = true

# Number of parallel requests to execute concurrently (1-100)
# Higher values may hit provider rate limits
# Recommended: 5-10 for most use cases
parallel_requests = 5

# Whether to continue benchmarking after a test fails
# true = continue running remaining tests
# false = stop on first failure
continue_on_failure = true

# Optional: Seed for reproducible randomization
# Uncomment to enable deterministic behavior:
# random_seed = 42

# ============================================================================
# Evaluation Configuration
# ============================================================================
# Defines which metrics to compute and how to compute them

[evaluation]
# List of metrics to compute
# Available metrics:
#   - "perplexity": Language model prediction quality (lower is better)
#   - "faithfulness": Factual accuracy and hallucination detection
#   - "relevance": Task/prompt alignment scoring
#   - "coherence": Output fluency and logical consistency
#   - "latency": Response time measurement (P50, P95, P99)
#   - "token_efficiency": Input/output token usage analysis
metrics = ["perplexity", "faithfulness", "relevance", "latency"]

# Model to use for LLM-as-judge evaluations
# This model is used for qualitative assessments like faithfulness and relevance
# Recommended: gpt-4, claude-3-opus-20240229
llm_judge_model = "gpt-4"

# Optional: Provider to use for LLM-as-judge
# If not specified, uses the first available provider
llm_judge_provider = "openai"

# Confidence threshold for passing evaluations (0.0 - 1.0)
# Tests with scores below this threshold are marked as failed
confidence_threshold = 0.7

# Whether to include detailed evaluation explanations
# true = include reasoning for each metric score
# false = only include scores
include_explanations = true

# ============================================================================
# Global Overrides (Optional)
# ============================================================================
# These settings override provider-specific values globally

# Global timeout in seconds (overrides provider-specific timeouts)
# Uncomment to enable:
# global_timeout_seconds = 60

# ============================================================================
# Environment Variable Examples
# ============================================================================
# You can override any setting using environment variables with the
# LLM_TEST_BENCH_ prefix and double underscores for nesting:
#
# Provider settings:
#   export LLM_TEST_BENCH_PROVIDERS__OPENAI__DEFAULT_MODEL="gpt-4"
#   export LLM_TEST_BENCH_PROVIDERS__OPENAI__TIMEOUT_SECONDS=60
#   export LLM_TEST_BENCH_PROVIDERS__ANTHROPIC__MAX_RETRIES=5
#
# Benchmark settings:
#   export LLM_TEST_BENCH_BENCHMARKS__PARALLEL_REQUESTS=10
#   export LLM_TEST_BENCH_BENCHMARKS__SAVE_RESPONSES=false
#   export LLM_TEST_BENCH_BENCHMARKS__OUTPUT_DIR="/tmp/results"
#
# Evaluation settings:
#   export LLM_TEST_BENCH_EVALUATION__METRICS="latency,faithfulness"
#   export LLM_TEST_BENCH_EVALUATION__LLM_JUDGE_MODEL="claude-3-opus"
#   export LLM_TEST_BENCH_EVALUATION__CONFIDENCE_THRESHOLD=0.8
#
# ============================================================================
# Quick Start
# ============================================================================
# 1. Copy this file to ~/.config/llm-test-bench/config.toml
#    mkdir -p ~/.config/llm-test-bench
#    cp config.example.toml ~/.config/llm-test-bench/config.toml
#
# 2. Set your API keys:
#    export OPENAI_API_KEY="sk-..."
#    export ANTHROPIC_API_KEY="sk-ant-..."
#
# 3. Customize the settings above as needed
#
# 4. Run llm-test-bench commands - configuration will be loaded automatically!
