// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
// https://www.apache.org/licenses/LICENSE-2.0> or the MIT license
// <LICENSE-MIT or https://opensource.org/licenses/MIT>, at your
// option. This file may not be copied, modified, or distributed
// except according to those terms.

//! Provider trait definition.
//!
//! This module defines the core `Provider` trait that all LLM provider
//! implementations must satisfy. The trait provides a consistent interface
//! for interacting with different LLM APIs.

use async_trait::async_trait;

use super::error::ProviderError;
use super::types::{CompletionRequest, CompletionResponse, ModelInfo, ResponseStream};

/// The core trait that all LLM providers must implement.
///
/// This trait defines the contract for interacting with LLM providers.
/// Implementations handle the provider-specific API details while presenting
/// a consistent interface to the rest of the application.
///
/// # Requirements
///
/// All implementations must be `Send + Sync` to support concurrent usage
/// across async tasks.
///
/// # Examples
///
/// Basic usage with a provider:
///
/// ```no_run
/// use llm_test_bench_core::providers::{Provider, CompletionRequest};
/// # use llm_test_bench_core::providers::OpenAIProvider;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// # let provider = OpenAIProvider::new("key".to_string());
/// let request = CompletionRequest::new("gpt-4", "Explain Rust ownership");
/// let response = provider.complete(request).await?;
/// println!("Response: {}", response.content);
/// # Ok(())
/// # }
/// ```
///
/// Streaming responses:
///
/// ```no_run
/// use llm_test_bench_core::providers::{Provider, CompletionRequest};
/// use futures::StreamExt;
/// # use llm_test_bench_core::providers::OpenAIProvider;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// # let provider = OpenAIProvider::new("key".to_string());
/// let request = CompletionRequest::new("gpt-4", "Write a story")
///     .with_streaming();
///
/// let mut stream = provider.stream(request).await?;
/// while let Some(chunk) = stream.next().await {
///     match chunk {
///         Ok(text) => print!("{}", text),
///         Err(e) => eprintln!("Error: {}", e),
///     }
/// }
/// # Ok(())
/// # }
/// ```
#[async_trait]
pub trait Provider: Send + Sync {
    /// Completes a prompt with the LLM and returns the full response.
    ///
    /// This method sends the completion request to the provider and waits
    /// for the complete response before returning. For incremental results,
    /// use [`stream`](Provider::stream) instead.
    ///
    /// # Arguments
    ///
    /// * `request` - The completion request containing the prompt and parameters
    ///
    /// # Returns
    ///
    /// Returns a `CompletionResponse` containing the generated text and metadata,
    /// or a `ProviderError` if the request fails.
    ///
    /// # Errors
    ///
    /// This method can return various errors including:
    /// - `ProviderError::InvalidApiKey` - Invalid or missing API key
    /// - `ProviderError::RateLimitExceeded` - Rate limit exceeded
    /// - `ProviderError::ContextLengthExceeded` - Prompt too long
    /// - `ProviderError::NetworkError` - Network communication failure
    /// - `ProviderError::ApiError` - Provider-specific API error
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, CompletionRequest, OpenAIProvider};
    /// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// let request = CompletionRequest::new("gpt-4", "What is Rust?")
    ///     .with_max_tokens(100)
    ///     .with_temperature(0.7);
    ///
    /// let response = provider.complete(request).await?;
    /// println!("Generated {} tokens", response.usage.total_tokens);
    /// # Ok(())
    /// # }
    /// ```
    async fn complete(&self, request: CompletionRequest) -> Result<CompletionResponse, ProviderError>;

    /// Streams completion tokens as they're generated by the model.
    ///
    /// This method initiates a streaming response where tokens are yielded
    /// incrementally as the model generates them. This provides a better user
    /// experience for long responses.
    ///
    /// # Arguments
    ///
    /// * `request` - The completion request (typically with `stream: true`)
    ///
    /// # Returns
    ///
    /// Returns a `ResponseStream` that yields text chunks as they're generated,
    /// or a `ProviderError` if the stream cannot be initiated.
    ///
    /// # Errors
    ///
    /// Can return the same errors as [`complete`](Provider::complete), plus:
    /// - Errors during stream processing are yielded as stream items
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, CompletionRequest, OpenAIProvider};
    /// # use futures::StreamExt;
    /// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// let request = CompletionRequest::new("gpt-4", "Write a poem")
    ///     .with_streaming();
    ///
    /// let mut stream = provider.stream(request).await?;
    /// while let Some(result) = stream.next().await {
    ///     match result {
    ///         Ok(chunk) => print!("{}", chunk),
    ///         Err(e) => eprintln!("Stream error: {}", e),
    ///     }
    /// }
    /// # Ok(())
    /// # }
    /// ```
    async fn stream(&self, request: CompletionRequest) -> Result<ResponseStream, ProviderError>;

    /// Returns information about all models supported by this provider.
    ///
    /// This method provides metadata about the models available through this
    /// provider, including context limits and feature support.
    ///
    /// # Returns
    ///
    /// A vector of `ModelInfo` structures describing supported models.
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, OpenAIProvider};
    /// # fn example() {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// let models = provider.supported_models();
    /// for model in models {
    ///     println!("{}: {} tokens", model.name, model.max_tokens);
    /// }
    /// # }
    /// ```
    fn supported_models(&self) -> Vec<ModelInfo>;

    /// Returns the maximum context length for a specific model.
    ///
    /// This method looks up the maximum number of tokens that can be processed
    /// by the specified model. Returns `None` if the model is not supported
    /// by this provider.
    ///
    /// # Arguments
    ///
    /// * `model` - The model identifier to query
    ///
    /// # Returns
    ///
    /// The maximum context length in tokens, or `None` if the model is unknown.
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, OpenAIProvider};
    /// # fn example() {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// if let Some(max_len) = provider.max_context_length("gpt-4") {
    ///     println!("GPT-4 supports up to {} tokens", max_len);
    /// }
    /// # }
    /// ```
    fn max_context_length(&self, model: &str) -> Option<usize>;

    /// Returns the name of this provider.
    ///
    /// This should be a lowercase identifier like "openai", "anthropic", etc.
    ///
    /// # Returns
    ///
    /// The provider name as a string slice.
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, OpenAIProvider};
    /// # fn example() {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// assert_eq!(provider.name(), "openai");
    /// # }
    /// ```
    fn name(&self) -> &str;

    /// Validates that the provider configuration is correct and usable.
    ///
    /// This method performs validation checks such as:
    /// - API key is present and in the correct format
    /// - Network connectivity to the provider
    /// - Authentication is successful
    ///
    /// This is useful for early validation before attempting actual requests.
    ///
    /// # Returns
    ///
    /// `Ok(())` if the configuration is valid, or a `ProviderError` describing
    /// what's wrong.
    ///
    /// # Errors
    ///
    /// Common errors:
    /// - `ProviderError::InvalidApiKey` - API key missing or malformed
    /// - `ProviderError::AuthenticationError` - API key is invalid
    /// - `ProviderError::NetworkError` - Cannot reach provider
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, OpenAIProvider};
    /// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// provider.validate_config().await?;
    /// println!("Provider is configured correctly");
    /// # Ok(())
    /// # }
    /// ```
    async fn validate_config(&self) -> Result<(), ProviderError>;

    /// Estimates the number of tokens in the given text for the specified model.
    ///
    /// This method provides an estimate of token count, which is useful for:
    /// - Validating requests won't exceed context limits
    /// - Estimating costs before making requests
    /// - Truncating prompts to fit within limits
    ///
    /// Note: This is typically an approximation. For exact counts, use the
    /// provider's tokenization API if available.
    ///
    /// # Arguments
    ///
    /// * `text` - The text to estimate tokens for
    /// * `model` - The model whose tokenization to use
    ///
    /// # Returns
    ///
    /// The estimated number of tokens, or an error if estimation fails.
    ///
    /// # Errors
    ///
    /// - `ProviderError::ModelNotFound` - Unknown model
    /// - `ProviderError::InternalError` - Estimation failed
    ///
    /// # Examples
    ///
    /// ```no_run
    /// # use llm_test_bench_core::providers::{Provider, OpenAIProvider};
    /// # fn example() -> Result<(), Box<dyn std::error::Error>> {
    /// # let provider = OpenAIProvider::new("key".to_string());
    /// let text = "Hello, world!";
    /// let tokens = provider.estimate_tokens(text, "gpt-4")?;
    /// println!("Estimated {} tokens", tokens);
    /// # Ok(())
    /// # }
    /// ```
    fn estimate_tokens(&self, text: &str, model: &str) -> Result<usize, ProviderError>;
}

/// A helper trait for provider implementations that support retries.
///
/// This trait is automatically implemented for any type that implements
/// `Provider` and provides retry functionality with exponential backoff.
#[async_trait]
pub trait RetryableProvider: Provider {
    /// Completes a request with automatic retry logic.
    ///
    /// This method wraps [`complete`](Provider::complete) with exponential
    /// backoff retry logic for retryable errors.
    ///
    /// # Arguments
    ///
    /// * `request` - The completion request
    /// * `max_retries` - Maximum number of retry attempts
    ///
    /// # Returns
    ///
    /// The completion response or an error if all retries are exhausted.
    async fn complete_with_retry(
        &self,
        request: CompletionRequest,
        max_retries: u32,
    ) -> Result<CompletionResponse, ProviderError> {
        let mut attempts = 0;

        loop {
            match self.complete(request.clone()).await {
                Ok(response) => return Ok(response),
                Err(e) if e.is_retryable() && attempts < max_retries => {
                    attempts += 1;
                    let delay = calculate_backoff(attempts, e.retry_delay());
                    tokio::time::sleep(delay).await;
                }
                Err(e) => return Err(e),
            }
        }
    }
}

// Blanket implementation for all Provider types
impl<T: Provider> RetryableProvider for T {}

/// Calculates the exponential backoff delay for a retry attempt.
///
/// # Arguments
///
/// * `attempt` - The attempt number (1-indexed)
/// * `suggested_delay` - Optional delay suggested by the provider
///
/// # Returns
///
/// The duration to wait before retrying.
///
/// # Examples
///
/// ```
/// use llm_test_bench_core::providers::traits::calculate_backoff;
/// use std::time::Duration;
///
/// let delay = calculate_backoff(1, None);
/// assert_eq!(delay, Duration::from_secs(1));
///
/// let delay = calculate_backoff(3, None);
/// assert_eq!(delay, Duration::from_secs(4)); // 2^2
///
/// let suggested = Some(Duration::from_secs(30));
/// let delay = calculate_backoff(1, suggested);
/// assert_eq!(delay, Duration::from_secs(30));
/// ```
pub fn calculate_backoff(attempt: u32, suggested_delay: Option<std::time::Duration>) -> std::time::Duration {
    use std::time::Duration;

    // Use suggested delay if provided
    if let Some(delay) = suggested_delay {
        return delay;
    }

    // Exponential backoff: 1s, 2s, 4s, 8s, 16s, 32s, 60s (capped)
    let base_delay_ms = 1000_u64;
    let max_delay_ms = 60_000_u64;

    let delay_ms = base_delay_ms * 2_u64.pow(attempt.saturating_sub(1));
    Duration::from_millis(delay_ms.min(max_delay_ms))
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;

    #[test]
    fn test_calculate_backoff() {
        assert_eq!(calculate_backoff(1, None), Duration::from_secs(1));
        assert_eq!(calculate_backoff(2, None), Duration::from_secs(2));
        assert_eq!(calculate_backoff(3, None), Duration::from_secs(4));
        assert_eq!(calculate_backoff(4, None), Duration::from_secs(8));
        assert_eq!(calculate_backoff(5, None), Duration::from_secs(16));
        assert_eq!(calculate_backoff(6, None), Duration::from_secs(32));
        assert_eq!(calculate_backoff(7, None), Duration::from_secs(60));
        assert_eq!(calculate_backoff(10, None), Duration::from_secs(60)); // Capped
    }

    #[test]
    fn test_calculate_backoff_with_suggested() {
        let suggested = Some(Duration::from_secs(45));
        assert_eq!(calculate_backoff(1, suggested), Duration::from_secs(45));
        assert_eq!(calculate_backoff(5, suggested), Duration::from_secs(45));
    }
}
